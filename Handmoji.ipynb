{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce39a272-ea97-4ad5-bd96-d63116f56809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /Users/alexandertekle/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /Users/alexandertekle/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from opencv-python) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "656ad770-c53b-4a86-8cba-96d66ccb9f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class HandGestureDataset(Dataset):\n",
    "    def __init__(self, json_file, img_dir, transform=None):\n",
    "        with open(json_file, 'r') as f:\n",
    "            self.annotations = json.load(f)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.classes = list(set(item['gesture'] for item in self.annotations.values()))\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = list(self.annotations.keys())[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        annotation = self.annotations[img_name]\n",
    "        gesture = annotation['gesture']\n",
    "        bbox = annotation['bbox']\n",
    "\n",
    "        label = self.class_to_idx[gesture]\n",
    "        bbox = torch.tensor(bbox, dtype=torch.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label, bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fb5ae6e-afc1-4fa1-bad6-d35627d3485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create dataset\n",
    "dataset = HandGestureDataset('hand_gesture_data/augmented_annotations.json', 'hand_gesture_data', transform=transform)\n",
    "\n",
    "# Split dataset into train and validation sets\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d67c40b-3e23-4ca9-bd4f-adaafec4a251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class HandGestureModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(HandGestureModel, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        num_features = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Identity()\n",
    "        self.fc1 = nn.Linear(num_features, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.fc3 = nn.Linear(128, 4)  # for bounding box regression\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.resnet(x)\n",
    "        features = torch.relu(self.fc1(features))\n",
    "        class_output = self.fc2(features)\n",
    "        bbox_output = self.fc3(features)\n",
    "        return class_output, bbox_output\n",
    "\n",
    "model = HandGestureModel(num_classes=len(dataset.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9abe9b9-232b-48fe-b606-89a53daa6ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 322737.5295, Val Loss: 309184.2773, Val Acc: 31.25%\n",
      "Epoch 2/50, Train Loss: 288092.0582, Val Loss: 258677.0039, Val Acc: 18.75%\n",
      "Epoch 3/50, Train Loss: 214690.9592, Val Loss: 168053.3867, Val Acc: 43.75%\n",
      "Epoch 4/50, Train Loss: 121329.3099, Val Loss: 72595.4902, Val Acc: 31.25%\n",
      "Epoch 5/50, Train Loss: 65421.3485, Val Loss: 55393.1113, Val Acc: 31.25%\n",
      "Epoch 6/50, Train Loss: 54744.6304, Val Loss: 66017.6914, Val Acc: 18.75%\n",
      "Epoch 7/50, Train Loss: 54232.6587, Val Loss: 53090.4839, Val Acc: 31.25%\n",
      "Epoch 8/50, Train Loss: 52706.7680, Val Loss: 53972.6699, Val Acc: 18.75%\n",
      "Epoch 9/50, Train Loss: 51679.2747, Val Loss: 49044.0352, Val Acc: 25.00%\n",
      "Epoch 10/50, Train Loss: 50396.9794, Val Loss: 51540.1655, Val Acc: 31.25%\n",
      "Epoch 11/50, Train Loss: 49324.1430, Val Loss: 47987.8833, Val Acc: 31.25%\n",
      "Epoch 12/50, Train Loss: 46401.5046, Val Loss: 43862.0366, Val Acc: 18.75%\n",
      "Epoch 13/50, Train Loss: 42978.9569, Val Loss: 37859.8203, Val Acc: 12.50%\n",
      "Epoch 14/50, Train Loss: 40562.3327, Val Loss: 32545.0815, Val Acc: 31.25%\n",
      "Epoch 15/50, Train Loss: 35893.2311, Val Loss: 31749.4282, Val Acc: 18.75%\n",
      "Epoch 16/50, Train Loss: 33553.4437, Val Loss: 27024.6816, Val Acc: 18.75%\n",
      "Epoch 17/50, Train Loss: 30438.3738, Val Loss: 23425.7930, Val Acc: 31.25%\n",
      "Epoch 18/50, Train Loss: 29854.1871, Val Loss: 21494.9238, Val Acc: 31.25%\n",
      "Epoch 19/50, Train Loss: 27548.7304, Val Loss: 19533.6934, Val Acc: 31.25%\n",
      "Epoch 20/50, Train Loss: 26135.9865, Val Loss: 18854.6575, Val Acc: 31.25%\n",
      "Epoch 21/50, Train Loss: 24975.8096, Val Loss: 18193.3840, Val Acc: 18.75%\n",
      "Epoch 22/50, Train Loss: 25855.0119, Val Loss: 20112.9580, Val Acc: 6.25%\n",
      "Epoch 23/50, Train Loss: 22225.1469, Val Loss: 17607.9937, Val Acc: 37.50%\n",
      "Epoch 24/50, Train Loss: 21920.9033, Val Loss: 16111.4180, Val Acc: 6.25%\n",
      "Epoch 25/50, Train Loss: 21318.5195, Val Loss: 18489.6528, Val Acc: 25.00%\n",
      "Epoch 26/50, Train Loss: 20647.9802, Val Loss: 16956.3447, Val Acc: 12.50%\n",
      "Epoch 27/50, Train Loss: 22678.8002, Val Loss: 15947.0164, Val Acc: 18.75%\n",
      "Epoch 28/50, Train Loss: 18890.4167, Val Loss: 15283.9761, Val Acc: 6.25%\n",
      "Epoch 29/50, Train Loss: 18738.5686, Val Loss: 19099.1345, Val Acc: 12.50%\n",
      "Epoch 30/50, Train Loss: 18100.5835, Val Loss: 13857.1882, Val Acc: 12.50%\n",
      "Epoch 31/50, Train Loss: 16802.4327, Val Loss: 14153.3198, Val Acc: 25.00%\n",
      "Epoch 32/50, Train Loss: 16741.2464, Val Loss: 16953.2188, Val Acc: 25.00%\n",
      "Epoch 33/50, Train Loss: 13661.4380, Val Loss: 10660.9078, Val Acc: 12.50%\n",
      "Epoch 34/50, Train Loss: 11696.1482, Val Loss: 9728.1117, Val Acc: 0.00%\n",
      "Epoch 35/50, Train Loss: 12432.8100, Val Loss: 11622.0359, Val Acc: 31.25%\n",
      "Epoch 36/50, Train Loss: 9876.0012, Val Loss: 9947.8989, Val Acc: 25.00%\n",
      "Epoch 37/50, Train Loss: 8862.9347, Val Loss: 7880.3927, Val Acc: 18.75%\n",
      "Epoch 38/50, Train Loss: 8718.2063, Val Loss: 8182.9020, Val Acc: 25.00%\n",
      "Epoch 39/50, Train Loss: 9231.2184, Val Loss: 5527.1384, Val Acc: 12.50%\n",
      "Epoch 40/50, Train Loss: 5925.6363, Val Loss: 6446.2930, Val Acc: 37.50%\n",
      "Epoch 41/50, Train Loss: 6504.9599, Val Loss: 6983.2461, Val Acc: 12.50%\n",
      "Epoch 42/50, Train Loss: 6668.8776, Val Loss: 6395.0151, Val Acc: 31.25%\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define loss functions and optimizer\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "criterion_bbox = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "device = torch.device(\"mps\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels, bboxes in train_loader:\n",
    "        images, labels, bboxes = images.to(device), labels.to(device), bboxes.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        class_outputs, bbox_outputs = model(images)\n",
    "        loss_cls = criterion_cls(class_outputs, labels)\n",
    "        loss_bbox = criterion_bbox(bbox_outputs, bboxes)\n",
    "        loss = loss_cls + loss_bbox\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels, bboxes in val_loader:\n",
    "            images, labels, bboxes = images.to(device), labels.to(device), bboxes.to(device)\n",
    "            class_outputs, bbox_outputs = model(images)\n",
    "            loss_cls = criterion_cls(class_outputs, labels)\n",
    "            loss_bbox = criterion_bbox(bbox_outputs, bboxes)\n",
    "            loss = loss_cls + loss_bbox\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = class_outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_loader):.4f}, '\n",
    "          f'Val Loss: {val_loss/len(val_loader):.4f}, Val Acc: {100.*correct/total:.2f}%')\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'hand_gesture_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe3f47f-cb9d-4828-b9f6-84711db06d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ea7c99-c7f6-4ed4-b0ae-9fd5563c6798",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
