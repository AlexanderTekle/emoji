{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10adea9a-20e5-4d52-8193-7a8f3ea50d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hand_gesture_data/thumbs_up_0.jpg\n",
      "[317, 348, 575, 641]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 1/16 [00:02<00:33,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hand_gesture_data/thumbs_up_1.jpg\n",
      "[398, 482, 555, 553]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 2/16 [00:03<00:22,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hand_gesture_data/thumbs_up_2.jpg\n",
      "[226, 220, 642, 759]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 3/16 [00:05<00:21,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hand_gesture_data/open_palm_0.jpg\n",
      "[302, 314, 567, 682]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 4/16 [00:07<00:22,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hand_gesture_data/open_palm_1.jpg\n",
      "[265, 335, 628, 659]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 5/16 [00:07<00:13,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hand_gesture_data/open_palm_2.jpg\n",
      "[179, 321, 818, 708]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 7/16 [00:12<00:15,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hand_gesture_data/closed_fist_0.jpg\n",
      "[354, 511, 446, 384]\n",
      "hand_gesture_data/closed_fist_1.jpg\n",
      "[359, 528, 459, 416]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 8/16 [00:12<00:09,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hand_gesture_data/closed_fist_2.jpg\n",
      "[278, 596, 521, 401]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 9/16 [00:12<00:06,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hand_gesture_data/peace_sign_0.jpg\n",
      "[389, 458, 507, 565]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 10/16 [00:14<00:06,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hand_gesture_data/peace_sign_1.jpg\n",
      "[399, 401, 576, 643]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 11/16 [00:15<00:05,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hand_gesture_data/peace_sign_2.jpg\n",
      "[416, 410, 559, 639]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 12/16 [00:17<00:05,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hand_gesture_data/pointing_finger_0.jpg\n",
      "[290, 488, 705, 504]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 13/16 [00:17<00:03,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hand_gesture_data/pointing_finger_1.jpg\n",
      "[403, 443, 474, 413]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 14/16 [00:18<00:01,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hand_gesture_data/pointing_finger_2.jpg\n",
      "[219, 419, 628, 368]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 15/16 [00:18<00:00,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hand_gesture_data/pointing_finger_3.jpg\n",
      "[296, 367, 590, 580]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:18<00:00,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentation complete. Total images: 160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load your annotations\n",
    "with open('hand_gesture_data/annotations.json', 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# Define augmentation pipeline\n",
    "transform = A.Compose([\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.Flip(p=0.5),\n",
    "    A.Transpose(p=0.5),\n",
    "    A.OneOf([\n",
    "        A.GaussNoise(p=0.9),\n",
    "        A.GaussNoise(),\n",
    "    ], p=0.2),\n",
    "    A.OneOf([\n",
    "        A.MotionBlur(p=0.2),\n",
    "        A.MedianBlur(blur_limit=3, p=0.1),\n",
    "        A.Blur(blur_limit=3, p=0.1),\n",
    "    ], p=0.2),\n",
    "    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=0.2),\n",
    "    A.OneOf([\n",
    "        A.OpticalDistortion(p=0.3),\n",
    "        A.GridDistortion(p=0.1),\n",
    "        A.PiecewiseAffine(p=0.3),\n",
    "    ], p=0.2),\n",
    "    A.OneOf([\n",
    "        A.CLAHE(clip_limit=2),\n",
    "        A.Sharpen(),\n",
    "        A.Emboss(),\n",
    "        A.RandomBrightnessContrast(),\n",
    "    ], p=0.3),\n",
    "    A.HueSaturationValue(p=0.3),\n",
    "], bbox_params=A.BboxParams(format='coco', label_fields=['class_labels']))\n",
    "\n",
    "# Function to augment a single image\n",
    "def augment_image(image, bbox, class_label, num_augmentations=9):\n",
    "    augmented_images = []\n",
    "    augmented_bboxes = []\n",
    "    \n",
    "    for _ in range(num_augmentations):\n",
    "        augmented = transform(image=image, bboxes=[bbox], class_labels=[class_label])\n",
    "        augmented_images.append(augmented['image'])\n",
    "        augmented_bboxes.append(augmented['bboxes'][0])\n",
    "    \n",
    "    return augmented_images, augmented_bboxes\n",
    "\n",
    "# Augment the dataset\n",
    "new_annotations = {}\n",
    "\n",
    "for img_name, annotation in tqdm(annotations.items()):\n",
    "    # Load the original image\n",
    "    img_path = os.path.join('hand_gesture_data', img_name)\n",
    "    print(img_path)\n",
    "    # if (img_path == \"hand_gesture_data/closed_fist_0.jpg\" or img_path == \"hand_gesture_data/closed_fist_1.jpg\"):\n",
    "    #     continue;\n",
    "    image = cv2.imread(img_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Get the original bbox and class\n",
    "    bbox = annotation['bbox']\n",
    "    print(bbox)\n",
    "    class_label = annotation['gesture']\n",
    "    \n",
    "    # Add the original image to new annotations\n",
    "    new_annotations[img_name] = annotation\n",
    "    \n",
    "    # Augment the image\n",
    "    aug_images, aug_bboxes = augment_image(image, bbox, class_label)\n",
    "    \n",
    "    # Save augmented images and add to new annotations\n",
    "    for i, (aug_image, aug_bbox) in enumerate(zip(aug_images, aug_bboxes)):\n",
    "        aug_img_name = f\"{os.path.splitext(img_name)[0]}_aug_{i}.jpg\"\n",
    "        aug_img_path = os.path.join('hand_gesture_data', aug_img_name)\n",
    "        \n",
    "        # Save the augmented image\n",
    "        cv2.imwrite(aug_img_path, cv2.cvtColor(aug_image, cv2.COLOR_RGB2BGR))\n",
    "        \n",
    "        # Add to new annotations\n",
    "        new_annotations[aug_img_name] = {\n",
    "            'gesture': class_label,\n",
    "            'bbox': list(map(int, aug_bbox))  # Convert float to int\n",
    "        }\n",
    "\n",
    "# Save the new annotations\n",
    "with open('hand_gesture_data/augmented_annotations.json', 'w') as f:\n",
    "    json.dump(new_annotations, f)\n",
    "\n",
    "print(f\"Augmentation complete. Total images: {len(new_annotations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4f8d75-42a2-4e93-8889-2ca02aa1a375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f712d679-19c5-4ad9-9669-7062e507b10b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
